{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10199c8-e3c0-43bf-9457-c76e01034ce6",
   "metadata": {},
   "source": [
    "Copyright 2024 Google, LLC. This software is provided as-is,\n",
    "without warranty or representation for any use or purpose. Your\n",
    "use of it is subject to your agreement with Google.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf3fcb-9e41-4105-b89f-64a573b3ffb3",
   "metadata": {},
   "source": [
    "# How to use Batch Predicitons with Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf06d4-3269-4327-9910-d42011859b1a",
   "metadata": {},
   "source": [
    "This notebook outlines how to interact with Vertex AI's Gemini batch predictions API. More info can be found at https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b923f-f712-48ec-a3b6-a3d2a305884f",
   "metadata": {},
   "source": [
    "## Prepare the python development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcedf7c6-18a7-4294-b8a8-11555a1bde48",
   "metadata": {},
   "source": [
    "First, let's identify any project specific variables to customize this notebook to your GCP environment. Change YOUR_PROJECT_ID with your own GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a33169-d938-4185-8a08-d48cbee56958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = \"YOUR_PROJECT_ID\"\n",
    "location = \"global\"\n",
    "region = \"us-central1\"\n",
    "bq_source_dataset_id = \"parts_data\"\n",
    "bq_source_table = \"inventory\"\n",
    "bq_batch_dataset_id = \"gemini_batch_test\"\n",
    "bq_batch_table = \"batch_input_table\"\n",
    "model_ver = \"gemini-1.5-pro-001\"\n",
    "qa_model_ver = \"gemini-1.5-flash-001\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae61b6-9b4a-4949-987d-8f5129220759",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install any needed python modules from our requirements.txt file. Most Vertex Workbench environments include all the packages we'll be using, but if you are using an external Jupyter Notebook or require any additional packages for your own needs, you can simply add them to the included requirements.txt file an run the folloiwng commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73c882-f0bb-445a-914e-6702189b8e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd497d42-27ab-473f-abb9-d48912768d50",
   "metadata": {},
   "source": [
    "Update the google-cloud-aiplatform package to the latest version if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8418358-858b-4bc9-b8cf-7502b521bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d728c-a4a8-4ea7-af59-559c09318512",
   "metadata": {},
   "source": [
    "Now we will import all required modules. For our purpose, we will be utilizing the following:\n",
    "\n",
    "- vertexai - The primary library for working with the Vertex AI Platform on GCP \n",
    "- BatchPredictionJob - Used to submit and manage batch prediction jobs with Gemini\n",
    "- bigquery - Work with data stored in BigQuery\n",
    "- iPython.display - Render HTML and Markdown responses from the Gemini API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55800605-c9b1-40c9-8c12-fb17de2876a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from IPython.display import HTML, Markdown\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.batch_prediction import BatchPredictionJob\n",
    "from vertexai.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eec833-662f-4db4-b338-553857f99b59",
   "metadata": {},
   "source": [
    "## Create an example source table in BQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df1a12-495d-426d-828b-8bf4c6502159",
   "metadata": {},
   "source": [
    "First we need to create a source table in BigQuery. For this example, we will create a new dataset and table to store some inventory data related to automotive parts. The source inventory data will then be imported from the inventory.csv file included in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99512d4e-503d-4de9-a182-708a0d4eb605",
   "metadata": {},
   "source": [
    "Construct a BigQuery client object and set dataset_id to the ID of the dataset to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb475a6-727f-4056-90a9-6616808c93df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client(project_id)\n",
    "source_dataset_id = bq_source_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d148ba-c991-45a6-b8b2-bd3a84cc86cb",
   "metadata": {},
   "source": [
    "Construct a Dataset object to send to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872dfb0-ebb2-440f-85eb-ea8477c3ffac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a full Dataset object to create.\n",
    "dataset = bigquery.Dataset(f\"{project_id}.{source_dataset_id}\")\n",
    "\n",
    "# Specify the geographic location where the dataset should reside.\n",
    "dataset.location = region\n",
    "\n",
    "# Send the dataset to your Google Cloud Project\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)  # API request\n",
    "print(f\"Created dataset {dataset.project}.{dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad5fff-bd97-4331-a6b0-1103ed52994b",
   "metadata": {},
   "source": [
    "Create a source table for the example inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba071b9-4586-4d35-9a52-08e29a298abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set table_id to the ID of the table to create.\n",
    "table_id = f\"{project_id}.{source_dataset_id}.{bq_source_table}\"\n",
    "\n",
    "# Set the schema of the table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"vehicle_manufacturer\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"vehicle_model\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"part_name\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"part_number\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"part_description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "# Create the table\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table)  # Make an API request.\n",
    "print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a721932a-a00b-4121-aad6-7ad92c229889",
   "metadata": {},
   "source": [
    "Import the inventory.csv file to populate the new table with some example inventory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf7195-91d3-4551-ac8f-abd4f7c757c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set job config\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    ")\n",
    "\n",
    "# Open the local file\n",
    "with open(\"inventory.csv\", \"rb\") as source_file:\n",
    "    # Create and run the load job\n",
    "    job = client.load_table_from_file(\n",
    "        source_file,\n",
    "        table_id,\n",
    "        job_config=job_config,\n",
    "    )\n",
    "\n",
    "    job.result()  # Wait for the load job to complete\n",
    "\n",
    "# Print the number of rows loaded\n",
    "table = client.get_table(table_id)\n",
    "print(f\"Loaded {table.num_rows} rows to {table_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad097bba-7998-4295-96e8-dd1f0469bcae",
   "metadata": {},
   "source": [
    "## Create the Batch Input Dataset and Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b0cba-364c-4170-8723-2f1b20005d59",
   "metadata": {},
   "source": [
    "We will now create a new dataset and table for the batch prediction job. Batch predictions are a way to efficiently send multiple multimodal prompts that are not latency sensitive. Unlike online prediction, where you are limited to one input prompt at a time, you can send a large number of multimodal prompts in a single batch request. Then, your responses asynchronously populate in your BigQuery storage output location. More information can be found online at https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e15165-4b72-4fa5-bb1f-0d54a59adc70",
   "metadata": {},
   "source": [
    "Define the BQ Dataset for the batch prediction job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00535e8-cd15-47e3-b0fe-e790c21bb9ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_dataset_id = bq_batch_dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a5abc-61fe-451b-8ff9-86ac1f66fb46",
   "metadata": {},
   "source": [
    "Construct a Dataset object to create, specify the region to store the data in and send the request to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e85b2-9140-4473-84c4-fd3b7558a0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = bigquery.Dataset(f\"{project_id}.{batch_dataset_id}\")\n",
    "\n",
    "# Specify the geographic location where the dataset should reside.\n",
    "dataset.location = region\n",
    "\n",
    "# Send the dataset to your Google Cloud Project\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)  # API request\n",
    "print(f\"Created dataset {dataset.project}.{dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3215c-c712-41bb-9270-b8759eb99957",
   "metadata": {},
   "source": [
    "Create the batch input table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412298b-7eb3-4894-abfb-5e0e0d60d00c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set table_id to the ID of the table to create.\n",
    "table_id = f\"{project_id}.{batch_dataset_id}.{bq_batch_table}\"\n",
    "\n",
    "# Set the schema of the table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"request\", \"STRING\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "# Create the table\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table)  # Make an API request.\n",
    "print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ff6af-1b4c-4696-9466-4c4c2213b7d3",
   "metadata": {},
   "source": [
    "Parse the source inventory table and create an entry in the batch prediction table for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5984cc-6c56-4f59-bb87-998005b56ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table IDs\n",
    "inventory_table_id = f\"{project_id}.{bq_source_dataset_id}.{bq_source_table}\"\n",
    "batch_input_table_id = f\"{project_id}.{bq_batch_dataset_id}.{bq_batch_table}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d56e3-65e4-4597-82d6-e784c2342e54",
   "metadata": {},
   "source": [
    "Deinfe the SQL query to fetch data and format the request string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb173c0-0c9d-4b8c-a06f-da4ed4d311cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SQL query to prepare data for the destination table\n",
    "# SQL query to read data from the source table\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    vehicle_manufacturer, \n",
    "    vehicle_model, \n",
    "    part_name \n",
    "FROM \n",
    "    `{inventory_table_id}`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd86a0-10d4-4606-8fa9-0800db0d4f15",
   "metadata": {
    "tags": []
   },
   "source": [
    "Run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb066c1-8104-4c5c-93fa-0460bd18b9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()\n",
    "\n",
    "for row in results:\n",
    "    # Create the JSON structure for each row\n",
    "    data = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": {\n",
    "                    \"text\": f\"Write an SEO optimized text for a Product Listing Page of 300 to 400 words. Keep the following points in mind: Main subject & main keyword: {row.vehicle_manufacturer} {row.vehicle_model} auto parts. Sub keywords: buy {row.vehicle_model} {row.part_name}, order {row.vehicle_model} {row.part_name} online. Written for the following website: https://my_autoparts.com/. Make sure the headers are not too similar and write it in HTML.\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"system_instruction\": {\n",
    "            \"parts\": [{\"text\": \"You are an SEO engineer, specializing in generating content for search engine optimization.\"}]\n",
    "        },\n",
    "        \"generation_config\": {\"top_k\": 5}\n",
    "    }\n",
    "\n",
    "    # Convert the Python dictionary to a JSON string\n",
    "    json_data = json.dumps(data)\n",
    "\n",
    "    # Insert the JSON data into the destination table\n",
    "    errors = client.insert_rows_json(batch_input_table_id, [{\"request\": json_data}])\n",
    "    if errors == []:\n",
    "        continue\n",
    "        #print(\"New row inserted.\")\n",
    "    else:\n",
    "        print(f\"Encountered errors while inserting row: {errors}\")\n",
    "        \n",
    "print('Rows inserted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648cbeb8-5079-49cc-bef8-775f7cf38256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "895260a3-8502-4192-9d20-0ba60c0c4cb8",
   "metadata": {},
   "source": [
    "## Define and submit a Batch Prediction job for Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aecf071-66f4-469e-be8a-405ba77a5f3b",
   "metadata": {},
   "source": [
    "Initialize vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e706b-b025-476b-8af7-9592cdcfb097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertexai.init(project=project_id, location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24858e20-84b2-47a9-a099-e7dbd395fb32",
   "metadata": {},
   "source": [
    "Next we'll create the Gemini batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d50cd2-becd-46ab-b75b-a58f86918f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = BatchPredictionJob.submit(\n",
    "    model_ver,   # source_model \n",
    "    #\"gs://rkiles-test/gemini-batch/batch_data2.json\", # input URI if using GCS\n",
    "    input_dataset = f'bq://{project_id}.{bq_batch_dataset_id}.{bq_batch_table}',  # input dataset if using BQ\n",
    "    output_uri_prefix = f'bq://{project_id}.{bq_batch_dataset_id}'  # This will generate a new output table in BQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b59a77-48e6-4f2c-906a-5fb724be7cae",
   "metadata": {},
   "source": [
    "View and monitor the job status. You can also view the status in the GCP Cloud Console under Vertex AI -> Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19999743-9576-49c0-8b0b-67f56d433948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check job status\n",
    "print(f\"Job resouce name: {job.resource_name}\")\n",
    "print(f\"Model resource name with the job: {job.model_name}\")\n",
    "print(f\"Job state: {job.state.name}\")\n",
    "\n",
    "# Refresh the job until complete\n",
    "while not job.has_ended:\n",
    "  time.sleep(5)\n",
    "  job.refresh()\n",
    "\n",
    "# Check if the job succeeds\n",
    "if job.has_succeeded:\n",
    "  print(\"Job succeeded!\")\n",
    "else:\n",
    "  print(f\"Job failed: {job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a3bd5-fc35-41b0-8884-d0331d4bef5e",
   "metadata": {},
   "source": [
    "Check the location of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18c7a8-c4e8-4002-bf95-36def8fb580f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Job output location: {job.output_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be479899-bec2-4bb9-b8ec-bce884ae4ada",
   "metadata": {
    "tags": []
   },
   "source": [
    "Capture the output table of the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b899c0a-ffb1-4b60-8a74-949c765fa048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_table = job.output_location.split(\".\")[-1]\n",
    "print(output_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c43ba-50c3-40e7-998d-300bd555c26b",
   "metadata": {},
   "source": [
    "List all the GenAI batch prediction jobs under the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04422a0-cb6b-464a-8650-7a610a78c703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for bpj in BatchPredictionJob.list():\n",
    "  #print(f\"Job ID: '{bpj.name}', Job state: {bpj.state.name}, Job model: {bpj.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770059cf-7c73-4b44-af9b-8871467eb5e5",
   "metadata": {},
   "source": [
    "## Print the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c4039-87eb-498f-bc06-1b205f1e6d27",
   "metadata": {},
   "source": [
    "Let's print the response from the batch predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed98d3f-455a-4558-9c9b-1f6c06a9bf8a",
   "metadata": {},
   "source": [
    "We will start by defining a function that we can use to rate and verify the quality of the generated results from the batch prediction job. We can specify a different model from the one used for the batch prediction. In this example, we used gemini-1.5-pro-002 for the predictions and gemini-1.5-flash-001 for rating and QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e96615-250e-41f9-8c2d-9ce41257c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get user rating\n",
    "def get_rating(source):\n",
    "    model = GenerativeModel(\n",
    "        model_name=qa_model_ver,\n",
    "        system_instruction=[\n",
    "            \"You are a professional Search Engine Optimization engineer.\",\n",
    "            \"You specialize in creating content that is optimized for search engine results.\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    response = model.generate_content(\n",
    "        f'''<OBJECTIVE>Rank the following content for SEO quality using a system of 1-10 with 1 being the lowest and 10 being the highest. Provide reasoning for your ranking.</OBJECTIVE> {source}'''\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ec7fe-f0f2-41f2-8178-41193d4f178c",
   "metadata": {},
   "source": [
    "Specify the output table created by the batch prediciton job to parse and define the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb81253-1b6b-4f11-bff3-f0847baaad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table_id = f'{bq_batch_dataset_id}.{output_table}'\n",
    "\n",
    "# SQL query to read data from the output table\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{output_table_id}`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9338d-47ab-4273-9a06-12805b66a865",
   "metadata": {},
   "source": [
    "Create the query client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06714b7-adc2-426b-b915-c5927be0858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query)\n",
    "results = query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07180720-813a-47c5-8e29-35d6c9595ce1",
   "metadata": {},
   "source": [
    "Run a for loop to print the output of the batch process and pause between each result. Press Enter to continue, 'r' to review/rate the response (this calls the get_rating function we created earlier) or the 'q' button to quit the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650acee-8a27-4cd8-a6fd-a72b191bd165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the results\n",
    "for row in results:\n",
    "    # Load the JSON string from the 'response' column\n",
    "    response_data = json.loads(row.response)\n",
    "\n",
    "    # Extract the generated text\n",
    "    generated_text = response_data[0]['content']['parts'][0]['text']\n",
    "\n",
    "    # Display the generated HTML\n",
    "    #display(HTML(f\"<strong>Request:</strong> {row.request}<br>\"))\n",
    "    print(f\"**Generated Text:**\\n\")\n",
    "    display(Markdown(generated_text))  # Use Markdown for rendering\n",
    "    print(\"-\" * 50)  # Add a separator between iterations  # Add a separator between iterations\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Press Enter for next, 'r' to review/rate, or 'q' to quit: \")\n",
    "        if user_input.lower() == 'q':\n",
    "            break  # Exit the loop if the user presses 'q'\n",
    "        elif user_input.lower() == 'r':\n",
    "            rating = get_rating(generated_text)\n",
    "            display(Markdown(rating))\n",
    "            # TODO: Store the rating in your database or use it as needed\n",
    "            break\n",
    "        else: \n",
    "            break  # Continue to the next iteration if Enter is pressed\n",
    "\n",
    "    if user_input.lower() == 'q':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1100c-dadb-45bd-892d-f34f5a6e5582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
